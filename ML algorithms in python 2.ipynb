{
    "cells": [
        {
            "cell_type": "code", 
            "execution_count": 1, 
            "metadata": {}, 
            "source": "# scipy\nimport scipy\nprint('scipy: {}'.format(scipy.__version__))\n# numpy\nimport numpy\nprint('numpy: {}'.format(numpy.__version__))\n# matplotlib\nimport matplotlib\nprint('matplotlib: {}'.format(matplotlib.__version__))\n# pandas\nimport pandas\nprint('pandas: {}'.format(pandas.__version__))\n# scikit-learn\nimport sklearn\nprint('sklearn: {}'.format(sklearn.__version__))", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "scipy: 0.17.0\nnumpy: 1.13.1\nmatplotlib: 1.5.0\npandas: 0.17.1\nsklearn: 0.19.1\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 2, 
            "metadata": {
                "scrolled": true
            }, 
            "source": "!rm -f pimaIndians.csv\n!wget https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data -O pimaIndians.csv", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "--2018-01-27 23:43:38--  https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\nResolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\nConnecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 23279 (23K) [text/plain]\nSaving to: \u2018pimaIndians.csv\u2019\n\n100%[======================================>] 23,279      --.-K/s   in 0.03s   \n\n2018-01-27 23:43:38 (684 KB/s) - \u2018pimaIndians.csv\u2019 saved [23279/23279]\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 3, 
            "metadata": {}, 
            "source": "!rm -f housing.csv\n!wget https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data -O housing.csv", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "--2018-01-27 23:43:38--  https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\nResolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\nConnecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 49082 (48K) [text/plain]\nSaving to: \u2018housing.csv\u2019\n\n100%[======================================>] 49,082      --.-K/s   in 0.07s   \n\n2018-01-27 23:43:40 (729 KB/s) - \u2018housing.csv\u2019 saved [49082/49082]\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 4, 
            "metadata": {}, 
            "source": "!pip install -U scikit-learn", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Requirement already up-to-date: scikit-learn in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sa0d-218294071c3031-cf228a6161b9/.local/lib/python2.7/site-packages\r\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Chap Evaluating the performance of ML algorithms\n    Classificaiton PIMA indiands\n    regression boston housing data set", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": 5, 
            "metadata": {}, 
            "source": "# Cross Validation Classification Accuracy\nfrom pandas import read_csv\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfilename = 'pimaIndians.csv'\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(filename, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nscoring = 'accuracy'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"Accuracy:\", (results.mean(), results.std()))", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "('Accuracy:', (0.76951469583048526, 0.048410519245671947))\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 6, 
            "metadata": {}, 
            "source": "#Logarithmic loss\n#logloss\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nscoring = 'neg_log_loss'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"Logloss: %.3f (%.3f)\") % (results.mean(), results.std())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Logloss: -0.493 (0.047)\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 7, 
            "metadata": {}, 
            "source": "#Area under ROC: receiver operating characteristic\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nscoring = 'roc_auc'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"AUC: %.3f (%.5f)\") % (results.mean(), results.std())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "AUC: 0.824 (0.04072)\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 8, 
            "metadata": {}, 
            "source": "from sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\ntest_size = 0.33\nseed = 7\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\npredicted = model.predict(X_test)\nmatrix = confusion_matrix(Y_test, predicted)\nprint(matrix)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[[141  21]\n [ 41  51]]\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 9, 
            "metadata": {}, 
            "source": "from sklearn.metrics import classification_report\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\ntest_size = 0.33\nseed = 7\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\nrandom_state=seed)\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\npredicted = model.predict(X_test)\nreport = classification_report(Y_test, predicted)\nprint(report)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "             precision    recall  f1-score   support\n\n        0.0       0.77      0.87      0.82       162\n        1.0       0.71      0.55      0.62        92\n\navg / total       0.75      0.76      0.75       254\n\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 10, 
            "metadata": {}, 
            "source": "# 0 and 1 is the classificaiton", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## regression metrics \n    using the boston housing data set", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 13, 
            "metadata": {}, 
            "source": "#MAE mean absoliute error\": cancelsout.. and hence advised to use RMSE\nfrom sklearn.linear_model import LinearRegression\nfilename = 'housing.csv'\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n'B', 'LSTAT', 'MEDV']\ndataframe = read_csv(filename, delim_whitespace=True, names=names)\narray = dataframe.values\nX = array[:,0:13]\nY = array[:,13]\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LinearRegression()\nscoring = 'neg_mean_absolute_error'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"MAE: %.3f (%.3f)\") % (results.mean(), results.std())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "MAE: -4.005 (2.084)\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 14, 
            "metadata": {}, 
            "source": "filename = 'housing.csv'\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n'B', 'LSTAT', 'MEDV']\ndataframe = read_csv(filename, delim_whitespace=True, names=names)\narray = dataframe.values\nX = array[:,0:13]\nY = array[:,13]\nnum_folds = 10\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LinearRegression()\nscoring = 'neg_mean_squared_error'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"MSE: %.3f (%.3f)\") % (results.mean(), results.std())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "MSE: -34.705 (45.574)\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 15, 
            "metadata": {}, 
            "source": "# R squared good ness of fit...\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LinearRegression()\nscoring = 'r2'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"R^2: %.3f (%.3f)\") % (results.mean(), results.std())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "R^2: 0.203 (0.595)\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 16, 
            "metadata": {}, 
            "source": "# kinda bad... range is from 0 to 1.. bad to good fit..", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "# spot checking of Classificaiton algorithms...", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "linear machine learning algorithms:  \n     Logistic Regression.  \n     Linear Discriminant Analysis.  \n       \nNonlinear machine learning algorithms:  \n     k-Nearest Neighbors.  \n     Naive Bayes.  \n     Classiffcation and Regression Trees.  \n     Support Vector Machines.  ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 17, 
            "metadata": {}, 
            "source": "filename = 'pimaIndians.csv'\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(filename, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\nnum_folds = 10\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.76951469583\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 18, 
            "metadata": {}, 
            "source": "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n#LDA: linear discriminant analysis\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LinearDiscriminantAnalysis()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n#using same values and data as above", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.773462064252\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 19, 
            "metadata": {}, 
            "source": "#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.726555023923\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 20, 
            "metadata": {}, 
            "source": "from sklearn.naive_bayes import GaussianNB\n# Naive Bayes\nmodel = GaussianNB()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.75517771702\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 21, 
            "metadata": {}, 
            "source": "from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.686107313739\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 22, 
            "metadata": {}, 
            "source": "type(results)", 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 22, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "numpy.ndarray"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 23, 
            "metadata": {}, 
            "source": "results[0:5]", 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "execution_count": 23, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array([ 0.63636364,  0.76623377,  0.67532468,  0.57142857,  0.63636364])"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 24, 
            "metadata": {}, 
            "source": "from sklearn.svm import SVC\nmodel = SVC()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.651025290499\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Spot checking on  regression ML algos\n    linear machine learning algorithms:  \n    Linear Regression.  \n    Ridge Regression.  \n    LASSO Linear Regression.  \n    Elastic Net Regression.  \n      \n    nonlinear machine learning algorithms:\n      K-Nearest Neighbors.  \n    Classiffcation and Regression Trees.  \n    Support Vector Machines.  \n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 25, 
            "metadata": {}, 
            "source": "from sklearn.linear_model import LinearRegression\nfilename = 'housing.csv'\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n'B', 'LSTAT', 'MEDV']\ndataframe = read_csv(filename, delim_whitespace=True, names=names)\narray = dataframe.values\nX = array[:,0:13]\nY = array[:,13]\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LinearRegression()\nscoring = 'neg_mean_squared_error'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "-34.7052559445\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 26, 
            "metadata": {}, 
            "source": "\nfrom sklearn.linear_model import Lasso\nmodel = Lasso()\nscoring = 'neg_mean_squared_error'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "-34.4640845883\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 27, 
            "metadata": {}, 
            "source": "from sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet()\nscoring = 'neg_mean_squared_error'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "-31.1645737142\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 28, 
            "metadata": {}, 
            "source": "#NON Linear classificaiton\nfrom sklearn.neighbors import KNeighborsRegressor\n \nmodel = KNeighborsRegressor()\nscoring = 'neg_mean_squared_error'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "-107.28683898\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 29, 
            "metadata": {}, 
            "source": "from sklearn.tree import DecisionTreeRegressor\nmodel = DecisionTreeRegressor()\nscoring = 'neg_mean_squared_error'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "-35.9838815686\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 30, 
            "metadata": {}, 
            "source": "from sklearn.svm import SVR\nmodel = SVR()\nscoring = 'neg_mean_squared_error'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "-91.0478243332\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "# comparing Ml algos", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "    The goal.. compare Ml algos consistently\n        Logistic Regression.\n        Linear Discriminant Analysis.\n        k-Nearest Neighbors.\n        ClassiFFcation and Regression Trees.\n        Naive Bayes.\n        Support Vector Machines.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 31, 
            "metadata": {}, 
            "source": "from matplotlib import pyplot\n\nfilename = 'pimaIndians.csv'\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(filename, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7)\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n    \n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "LR: 0.769515 (0.048411)\nLDA: 0.773462 (0.051592)\nKNN: 0.726555 (0.061821)\nCART: 0.691353 (0.060083)\nNB: 0.755178 (0.042766)\nSVM: 0.651025 (0.072141)\n", 
                    "name": "stdout"
                }, 
                {
                    "output_type": "display_data", 
                    "metadata": {}, 
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAESCAYAAAAG+ZUXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWQn9f8NnuegJieGgAAAAAElFTkSuQmCC\n", 
                        "text/plain": "<matplotlib.figure.Figure at 0x7f24512bc410>"
                    }
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 32, 
            "metadata": {}, 
            "source": "#LDA looks best...# highest score\n# LR looks great.. slightly lower score.. but better in terms of having lower variation..", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": 33, 
            "metadata": {}, 
            "source": "# R squared goodness of fit..\nscoring = 'r2'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"R^2: %.3f (%.3f)\") % (results.mean(), results.std())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "R^2: -0.554 (0.159)\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 34, 
            "metadata": {}, 
            "source": "# the value for R2 0 to 1.. for no fit to perfect fit.. the above then is not so good a fit... :(", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Spot check ML algoithms...\n    Linear :\n    Logistic Regression.\n    Linear Discriminant Analysis.\n    \n    nonlinear machine learning algorithms:\n     k-Nearest Neighbors.\n     Naive Bayes.\n     Classiffcation and Regression Trees.\n     Support Vector Machines.\n     \n     Spot check is a way of discivering which algo perfoms best..", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 35, 
            "metadata": {}, 
            "source": "\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\nnum_folds = 10\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.76951469583\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 36, 
            "metadata": {}, 
            "source": "# LDA\n# linear discrimininant analysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nX = array[:,0:8]\nY = array[:,8]\nnum_folds = 10\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LinearDiscriminantAnalysis()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.773462064252\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 37, 
            "metadata": {}, 
            "source": "from sklearn.neighbors import KNeighborsClassifier\n\n# Non LInear ML Classification algorithms\nkfold = KFold(n_splits=10, random_state=7)\nmodel = KNeighborsClassifier()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.726555023923\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 38, 
            "metadata": {}, 
            "source": "from sklearn.tree import DecisionTreeClassifier\n\nkfold = KFold(n_splits=10, random_state=7)\nmodel = DecisionTreeClassifier()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.696531100478\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 39, 
            "metadata": {}, 
            "source": "from sklearn.svm import SVC\nkfold = KFold(n_splits=10, random_state=7)\nmodel = SVC()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.651025290499\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Pipelines...\n    minimize data leakage\n    data prep and a modelling pipeline\n    feature extraciton and modelling pipeline", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 43, 
            "metadata": {}, 
            "source": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# load data\nfilename = 'pimaIndians.csv'\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(filename, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\n\n\n# create pipeline\nestimators = [] # estimators.. placeholder array\nestimators.append(('standardize', StandardScaler()))  # the purpose of this pipe.. to avoid leakage read below\nestimators.append(('lda', LinearDiscriminantAnalysis())) # keep appending what you want to add into the pipeline\nmodel = Pipeline(estimators)# casting..?\n\n# evaluate pipeline\n\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n\n# the issue of data leakage....\n#Data preparation is one easy way to leak knowledge of the whole training dataset to the algorithm. \n#For example, preparing your data using normalization or standardization on the entire training dataset before learning would not\n#be a valid test because the training dataset would have been influenced by the scale of the data in the test set\n\n# Pipelines help you prevent data leakage in your test harness by ensuring that data preparation like standardization is constrained to \n# each fold of your cross validation procedure. as in this following step\n# estimators.append(('standardize', StandardScaler())) ", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.773462064252\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 46, 
            "metadata": {}, 
            "source": "from sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\n# like the data pre.. standardization...\n# feature extraction must be restricted to the batch in process..\n# all the feature extraction and the feature union occurs\n# within each fold of the cross validation procedure.\nX = array[:,0:8]\nY = array[:,8]\n\n# create feature union\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=6)))\nfeature_union = FeatureUnion(features)\n\n# create pipeline\nestimators = []\nestimators.append(('feature_union', feature_union))\nestimators.append(('logistic', LogisticRegression()))\nmodel = Pipeline(estimators)\n# evaluate pipeline\nkfold = KFold(n_splits=10, random_state=7)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())\n", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.776042378674\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "## improve algorithms performance using ensembles..", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "    bagging ensemble methods such as bagged decision trees, random forest and extra trees.\n        Bagging: build models of the same type usually from different subsamples of the training data\n    boosting ensemble methods such as AdaBoost and stochastic gradient boosting.\n        boosting: build models of the same type, usually, each model tries to fix the prediction errors of the prior type\n    voting ensemble methods to combine the predictions from multiple algorithms.\n        simple statistics (like calculating the mean) are used to combine predictions", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 47, 
            "metadata": {}, 
            "source": "# Bagged decision Trees\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfilename = 'pimaIndians.csv'\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(filename, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\nseed = 7\nkfold = KFold(n_splits=10, random_state=seed)\ncart = DecisionTreeClassifier()\nnum_trees = 100\nmodel = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.770745044429\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 48, 
            "metadata": {}, 
            "source": "# if it was just the CART...\nkfold = KFold(n_splits=10, random_state=7)\nmodel = DecisionTreeClassifier()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.690037593985\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 49, 
            "metadata": {}, 
            "source": "# much improvement..", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": 50, 
            "metadata": {}, 
            "source": "# Random Forest Classification\n# random forests ~= bagged decision trees\n# Samples of the training dataset are taken with replacement, \n# but the trees are constructed in a way that reduces the correlation between individual classiffers\n\n# rather than greedily choosing the best split point in the construction of each tree, only a random subset of features are considered for each split.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nnum_trees = 100\nmax_features = 3\nkfold = KFold(n_splits=10, random_state=7)\nmodel = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.774675324675\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 51, 
            "metadata": {}, 
            "source": "# and that is a marginal improvement .. \n# over the Bagged decision trees algorithm...", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": 52, 
            "metadata": {}, 
            "source": "from sklearn.ensemble import ExtraTreesClassifier\nnum_trees = 100\nmax_features = 7\nkfold = KFold(n_splits=10, random_state=7)\nmodel = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.762918660287\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 53, 
            "metadata": {}, 
            "source": "# an extension of bagging where random trees are constructed from samples of the training dataset", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Boosting ensemble algorithms \n    creates a sequence of models that attempt to correct the mistakes\n    of the models before them in the sequence.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 54, 
            "metadata": {}, 
            "source": "# AdaBoost Classification\nfrom sklearn.ensemble import AdaBoostClassifier\nnum_trees = 30\nseed=7\nkfold = KFold(n_splits=10, random_state=seed)\nmodel = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.76045796309\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 55, 
            "metadata": {}, 
            "source": "# SGB ?GBM Stochastic gradient boosting... gradient boosting machines..\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nseed = 7\nnum_trees = 100\nkfold = KFold(n_splits=10, random_state=seed)\nmodel = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.764285714286\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# a marginally better result than adaboost", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": 56, 
            "metadata": {}, 
            "source": "# Voting Ensemble for Classification\n \nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\nkfold = KFold(n_splits=10, random_state=7)\n\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))\n\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nresults = cross_val_score(ensemble, X, Y, cv=kfold)\nprint(results.mean())", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.738209159262\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 57, 
            "metadata": {}, 
            "source": "# Voting ensemble.. gives.. lower accuracy than the usual..", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "\n## improving algorithm performance using tuning..\n    algorithm performance improvemen using parameter tuning\n    grid search using tuning strat\n    random search algorithm tuning strat", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "    Algorithm tuning aka  hyperparameter optimization where the algorithm parameters are referred to as hyperparameters, whereas the coe\u000ecients found by the \n    machine learning algorithm itself are referred to as parameters.\n    \n    Sci kit learn  \n    \n    Grid Search Parameter Tuning.\n    Random Search Parameter Tuning.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 58, 
            "metadata": {}, 
            "source": "# Grid Search for Algorithm Tuning\nimport numpy\nfrom pandas import read_csv\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfilename = 'pimaIndians.csv'\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = read_csv(filename, names=names)\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\n\n# learning rates..? \nalphas = numpy.array([1,0.1,0.01,0.001,0.0001,0])\nparam_grid = dict(alpha=alphas)\nmodel = Ridge()\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid.fit(X, Y)\nprint(grid.best_score_)\nprint(grid.best_estimator_.alpha)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.279617559313\n1.0\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 59, 
            "metadata": {}, 
            "source": "# Grid search.. class was used for a one dimensional grid search..\n# to find out the best parameter.. alpha.. for a ridge regression algorithm.. for this data set..", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": 62, 
            "metadata": {}, 
            "source": "# Randomized for Algorithm Tuning\nfrom scipy.stats import uniform\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_grid = {'alpha': uniform()}\nmodel = Ridge()\nrsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100,\nrandom_state=7)\nrsearch.fit(X, Y)\nprint(rsearch.best_score_)\n\nprint(rsearch.best_estimator_.alpha)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.279617127031\n0.977989511997\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 63, 
            "metadata": {}, 
            "source": "# sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations.", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": 64, 
            "metadata": {}, 
            "source": "#Running the example produces results much like those in the grid search example above. An optimal alpha value near 1.0 is discovered", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Saving and loading ML models for later use\n    pickle\n    joblib", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 66, 
            "metadata": {}, 
            "source": "from pickle import dump\nfrom pickle import load\n\n# Fit the model on 33%\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\n\n# save the model to disk\nfilename = 'finalized_model.sav'\ndump(model, open(filename, 'wb'))\n\n# some time later...\n\n# load the model from disk\nloaded_model = load(open(filename, 'rb'))\nresult = loaded_model.score(X_test, Y_test)\n\nprint(result)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.755905511811\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "code", 
            "execution_count": 68, 
            "metadata": {}, 
            "source": "# Save job using JobLib\n\nfrom sklearn.externals.joblib import dump\nfrom sklearn.externals.joblib import load\n\n# observe  load and dump are different from the above..\n\narray = dataframe.values\nX = array[:,0:8]\nY = array[:,8]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n# Fit the model on 33%\nmodel = LogisticRegression()\nmodel.fit(X_train, Y_train)\n# save the model to disk\nfilename = 'finalized_model.sav'\ndump(model, filename)\n# some time later...\n# load the model from disk\nloaded_model = load(filename)\nresult = loaded_model.score(X_test, Y_test)\nprint(result)", 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "0.755905511811\n", 
                    "name": "stdout"
                }
            ]
        }, 
        {
            "cell_type": "markdown", 
            "source": "    Manual Serialization. You might like to manually output the parameters of your\n    learned model so that you can use them directly in scikit-learn or another platform in\n    the future", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "", 
            "outputs": []
        }
    ], 
    "nbformat": 4, 
    "nbformat_minor": 1, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21"
        }
    }
}